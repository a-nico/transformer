{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3 # number of tokens\n",
    "d_vocab = 1000 # vocabulary size\n",
    "dm = 512 # \"model size\"\n",
    "h = 8 # number of \"heads\"\n",
    "dk = dm // h\n",
    "E = np.zeros((d_vocab, dm)) # embedding matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = np.zeros((n, d_vocab))\n",
    "\n",
    "# Embed tokens:\n",
    "x_embedded = x_input @ E # shape (n, dm)\n",
    "\n",
    "# Add \"positional encoding\"\n",
    "x_pos_encoded = x_embedded + positional_encoding(n, dm)\n",
    "\n",
    "def layer(x):\n",
    "    # \"Projection\" parameter matrices map Q, K, V to a lower dimensional space\n",
    "    WQ = [np.zeros((dm, dk)) for _ in range(h)]\n",
    "    WK = [np.zeros((dm , dk)) for _ in range(h)]\n",
    "    WV = [np.zeros((dm , dk)) for _ in range(h)]\n",
    "    \n",
    "    # Parameters for FFN\n",
    "    W1 = np.zeros((dm, 2048))\n",
    "    b1 = np.zeros(2048)\n",
    "    W2 = np.zeros((2048, dm))\n",
    "    b2 = np.zeros(dm)\n",
    "\n",
    "    # Sublayer 1: Multi-Head Attention\n",
    "    mha = multi_head_attention(x, x, x, WQ, WK, WV)\n",
    "    ffn_1 = ffn(mha, W1, W2, b1, b2)\n",
    "    sublayer1 = layer_norm(ffn_1 + x)\n",
    "\n",
    "    # Sublayer 2: Feed-Forward Network\n",
    "    ffn_2 = ffn(sublayer1, W1, W2, b1, b2)\n",
    "    sublayer2 = layer_norm(ffn_2 + sublayer1)\n",
    "    return sublayer2\n",
    "\n",
    "# repeat this N=6 times (6 layers)\n",
    "x = x_pos_encoded\n",
    "for _ in range(6):\n",
    "    x = layer(x)\n",
    "\n",
    "# now pass x to decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_norm(x):\n",
    "    # TODO: this is not defined in the paper.\n",
    "    return x\n",
    "\n",
    "def multi_head_attention(Q, K, V ,WQ, WK, WV):\n",
    "    heads = [attention(Q@WQ[i], K@WK[i], V@WV[i]) for i in range(len(WQ))]\n",
    "    return np.concatenate(heads, axis=1)    \n",
    "\n",
    "def softmax(x):\n",
    "    e = np.exp(x)\n",
    "    e /= np.expand_dims(np.sum(e, axis=1), 1)\n",
    "    return e\n",
    "\n",
    "def positional_encoding(n, dm):\n",
    "    PE = np.empty((n, dm))\n",
    "    t = np.array(range(n))\n",
    "    for i in range(dm // 2):\n",
    "        u = t / 10000**(2*i / dm)\n",
    "        PE[:, 2*i] = np.sin(u)\n",
    "        PE[:, 2*i+1] = np.cos(u)\n",
    "    return PE\n",
    "\n",
    "def attention(Q, K, V):\n",
    "    # equation 1 in paper\n",
    "    return softmax(Q @ K.T / np.sqrt(dk)) @ V\n",
    "\n",
    "def ffn(x, W1, W2, b1, b2):\n",
    "    return np.maximum(0, x @ W1 + b1) @ W2 + b2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
